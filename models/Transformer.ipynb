{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7508268,"sourceType":"datasetVersion","datasetId":4352274},{"sourceId":7541711,"sourceType":"datasetVersion","datasetId":4351877}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer\n\nI experimented with a LSTM and found that the LSTM is unable to provide better performance than train-based ensemble models. In fact, gradient boosting and random forest models are able to have a test set weighted ROC AUC at least .2 to .3 better than the LSTM. This leads me to the believe that there are some relationships in the test data that are being captured by my custom features that the LSTM is struggling to capture. I am going to try using a 2 layer Transformer Encoder to see if I can get better performance.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm \nimport torch\nimport torch.nn as nn\nimport torchtext\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom nltk.stem import SnowballStemmer\nimport re\nimport tensorflow as tf\nfrom sklearn.metrics import roc_auc_score\nimport math\n\ntqdm.pandas()\n\n# Getting the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the data\ntrain_data = pd.read_csv('../input/training-llm-competition/train.csv')\nvalid_data = pd.read_csv('../input/training-llm-competition/validation.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing ","metadata":{}},{"cell_type":"code","source":"# Creating a dictionary for the contradictions\n# Dictionary is from https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = {\n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the tokenizer\ntokenizer = get_tokenizer('spacy',language='en_core_web_sm')\n\n# Getting the stemmer\nstemmer = SnowballStemmer(language='english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function for preprocessing\ndef preprocess(essay:str):\n    preprocessed_essay = essay.lower()\n    \n    \n    # Iterating through the contractions and replacing the \n    for contraction in contractions.keys():\n        preprocessed_essay = re.sub(contraction.lower(),contractions[contraction].lower(),preprocessed_essay)\n    \n    # Subbing out \\n and \\t\n    preprocessed_essay = re.sub(\"\\n\",\"\",preprocessed_essay)\n    preprocessed_essay = re.sub(\"\\t\",\"\",preprocessed_essay)\n\n    # Replacing /xa0 = non-breaking space in Latin1\n    preprocessed_essay = preprocessed_essay.replace(u'\\xa0', u' ')\n    \n    final_preprocessed_essay = []\n    \n    # Running through tokenizer and returning the non-whitespace tokens\n    for token in tokenizer(preprocessed_essay):\n        temp_token = token.strip(\" \")\n        \n        if temp_token != \"\":\n            final_preprocessed_essay.append(stemmer.stem(token))\n    \n    return final_preprocessed_essay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running the training essays and validation essays through preprocessing\ntokenized_essays_train = train_data['essay'].progress_apply(preprocess)\ntokenized_essays_valid = valid_data['essay'].progress_apply(preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the vocab\nvocabulary = torch.load('../input/llm-competition-models/vocab.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to put each essay through the vocabulary\ndef put_through_vocab(essay:str) -> list:\n    return vocabulary(essay)\n\n# Indexed\nindexed_essays_train = [put_through_vocab(essay) for essay in tokenized_essays_train]\nindexed_essays_valid = [put_through_vocab(essay) for essay in tokenized_essays_valid]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Padding the essays\ntrain_padded = tf.keras.utils.pad_sequences(indexed_essays_train,maxlen=512,padding='post',truncating='post',value=vocabulary['<pad>'])\nvalid_padded = tf.keras.utils.pad_sequences(indexed_essays_valid,maxlen=512,padding='post',truncating='post',value=vocabulary['<pad>'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the dataloaders\nX_train_tensor = torch.from_numpy(train_padded)\ny_train_tensor = torch.from_numpy(train_data['LLM_written'].values)\nX_valid_tensor = torch.from_numpy(valid_padded)\ny_valid_tensor = torch.from_numpy(valid_data['LLM_written'].values)\ntraining_dataset = TensorDataset(X_train_tensor,y_train_tensor)\nvalid_dataset = TensorDataset(X_valid_tensor,y_valid_tensor)\ntraining_loader = DataLoader(training_dataset,batch_size=128,shuffle=True)\nvalid_loader = DataLoader(training_dataset,batch_size=128,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self,emb_size:int, dropout:float, maxlen:int = 500):\n        super(PositionalEncoding,self).__init__()\n        den = torch.exp(-torch.arange(0,emb_size,2)*math.log(10000) / emb_size)\n        pos = torch.arange(0,maxlen).reshape(maxlen,1)\n        pos_embedding = torch.zeros((maxlen,emb_size))\n        pos_embedding[:,0::2] = torch.sin(pos * den)\n        pos_embedding[:,1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(0)\n\n        self.dropout = nn.Dropout(dropout)\n\n        # Saving the positional encoding in the model state dict, but making sure PyTorch doesn't \"train\"\n        # these parameters because they don't need to be trained\n        self.register_buffer('pos_embedding',pos_embedding)\n\n    def forward(self,token_embedding):\n        return self.dropout(token_embedding + self.pos_embedding)\n\n# Transformer Model\nclass Model(nn.Module):\n    def __init__(self,vocab_size: int, emb_size:int,nheads:int,dim_feedforward:int,dropout:float,num_layers:int,max_length:int):\n        super().__init__()\n        self.embed_size = emb_size\n        self.embedding = nn.Embedding(vocab_size,emb_size,padding_idx=vocabulary['<pad>'])\n        self.positional_encoder = PositionalEncoding(emb_size,dropout,max_length)\n        self.encoder_layer = nn.TransformerEncoderLayer(emb_size,nheads,dim_feedforward,dropout,batch_first=True)\n        self.transformer = nn.TransformerEncoder(self.encoder_layer,num_layers)\n        self.fc1 = nn.Linear(emb_size,1)\n    \n    # Forward Function\n    def forward(self,X,src_key_padding_mask):\n        # Putting X through embedding\n        output = self.embedding(X.long()) * math.sqrt(self.embed_size)\n        output = self.positional_encoder(output)\n        \n        # Feeding through transformer encoder\n        output = self.transformer(output,src_key_padding_mask=src_key_padding_mask)\n        output = torch.mean(output,dim=1)\n        return nn.functional.sigmoid(self.fc1(output))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a mask to make sure the padding indicies are masked\ndef create_padding_mask(X):\n    return (X == vocabulary['<pad>'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the model\nmodel = Model(vocab_size=vocabulary.__len__(),emb_size=512,nheads=8,dim_feedforward=2048,dropout=0.2,num_layers=2,max_length=512)\n\n# Putting all on GPU\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the model training parameters\nEPOCHS = 100\nLEARNING_RATE = 1e-5\nLOSS = nn.BCELoss()\nOPTIMIZER = torch.optim.Adam(model.parameters(),LEARNING_RATE)\nhistory = []\nearly_stopping_threshold = 5\nbest_roc_auc = 0\ncurrent_count = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train_loss = 0\n    train_preds = None\n    valid_preds = None\n    train_targets = None\n    valid_targets = None\n    model.train()\n    for X,y in training_loader:\n        # Making predictions\n        y = y.to(torch.float32)\n        # Putting the tensors on the right device\n        X = X.to(device)\n        y = y.to(device)\n        \n        # Putting input through model\n        pred = model(X,src_key_padding_mask=create_padding_mask(X))\n        if train_preds is None:\n            train_preds = pred.cpu().detach().numpy()\n        else:\n            train_preds = np.append(train_preds,pred.cpu().detach().numpy(),axis=0)\n\n        # Getting the targets\n        if train_targets is None:\n            train_targets = y.cpu().numpy()\n        else:\n            train_targets = np.append(train_targets,y.cpu().detach().numpy(),axis=0)\n\n        # Getting the loss\n        loss = LOSS(pred,y.view(-1,1))\n\n        # Calculating the gradients\n        loss.backward()\n\n        # Taking a step with the optimizer\n        OPTIMIZER.step()\n\n        # Clear the gradients\n        OPTIMIZER.zero_grad()\n\n        # Adding the loss\n        train_loss += loss.item()\n\n    # Going through a validation loop\n    model.eval()\n    val_loss = 0\n    for X,y in valid_loader:\n        # Making predictions\n        X = X.to(device)\n        y = y.to(device)\n        y = y.to(torch.float32)\n\n        with torch.no_grad():\n            pred = model(X,src_key_padding_mask=create_padding_mask(X))\n            loss = LOSS(pred,y.view(-1,1))\n\n        val_loss += loss.item()\n        if valid_preds is None:\n            valid_preds = pred.cpu().detach().numpy()\n        else:\n            valid_preds = np.append(valid_preds,pred.cpu().detach().numpy(),axis=0)\n\n        # Getting the targets\n        if valid_targets is None:\n            valid_targets = y.cpu().numpy()\n        else:\n            valid_targets = np.append(valid_targets,y.cpu().detach().numpy(),axis=0)\n\n    # Early Stopping\n    if roc_auc_score(valid_targets,valid_preds) - best_roc_auc > 1e-3:\n        best_roc_auc = roc_auc_score(valid_targets,valid_preds)\n        count = 0\n\n        # Saving the best model & best embeddings \n        torch.save(model.state_dict(),'2-layer-transformer-encoder.pt')\n    else:\n        count += 1\n\n    # Appending the average example loss to the history\n    print(f'----------EPOCH {epoch} loss----------')\n    print(f'Train Loss: {train_loss / len(training_loader)}')\n    print(f'Valid Loss: {val_loss / len(valid_loader)}')\n    print(f'Training ROC AUC: {roc_auc_score(train_targets,train_preds)}')\n    print(f'Validation ROC AUC: {roc_auc_score(valid_targets,valid_preds)}')\n    history.append([train_loss / len(training_loader),val_loss / len(valid_loader),roc_auc_score(train_targets,train_preds),roc_auc_score(valid_targets,valid_preds)])\n    print('--------------------------------------')\n    print()\n\n    # Stopping the loop\n    if count == early_stopping_threshold:\n        print('Found no improvement!')\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history,columns=['Training Loss','Validation Loss','Training ROC AUC','Validation ROC AUC'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the loss\nhistory_df[['Training Loss','Validation Loss']].plot(title='Loss vs. Epochs',xlabel='Epochs',ylabel='Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the ROC AUC\nhistory_df[['Training ROC AUC','Validation ROC AUC']].plot(title='ROC AUC vs. Epochs',xlabel='Epochs',ylabel='ROC AUC')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the model\nmodel = Model(vocab_size=vocabulary.__len__(),emb_size=512,nheads=8,dim_feedforward=2048,dropout=0.2,num_layers=2,max_length=512)\nmodel.load_state_dict(torch.load('../working/2-layer-transformer-encoder.pt'))\n\n# Putting all on GPU\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference loop\nwith torch.no_grad():\n    model.eval()\n    train_preds = None\n    val_preds = None\n    train_targets = None\n    val_targets = None\n    for X,y in training_loader:\n        # Making predictions\n        X = X.to(device)\n        pred = model(X,src_key_padding_mask=create_padding_mask(X))\n        if train_preds is None:\n            train_preds = pred.cpu().detach().numpy()\n        else:\n            train_preds = np.append(train_preds,pred.cpu().detach().numpy(),axis=0)\n\n        # Getting the targets\n        if train_targets is None:\n            train_targets = y.cpu().numpy()\n        else:\n            train_targets = np.append(train_targets,y.cpu().detach().numpy(),axis=0)\n    for X,y in valid_loader:\n        # Making predictions\n        X = X.to(device)\n        pred = model(X,src_key_padding_mask=create_padding_mask(X))\n        if valid_preds is None:\n            valid_preds = pred.cpu().detach().numpy()\n        else:\n            valid_preds = np.append(valid_preds,pred.cpu().detach().numpy(),axis=0)\n\n        # Getting the targets\n        if valid_targets is None:\n            valid_targets = y.numpy()\n        else:\n            valid_targets = np.append(valid_targets,y.detach().numpy(),axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions\nprint('Predictions for Transformer')\ntrain_score = roc_auc_score(train_targets,train_preds)\nvalid_score = roc_auc_score(valid_targets,valid_preds)\nprint(f'Training ROC AUC: {train_score}')\nprint(f'Validation ROC AUC: {valid_score}')","metadata":{},"execution_count":null,"outputs":[]}]}