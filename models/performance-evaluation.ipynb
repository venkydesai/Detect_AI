{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Performance Evaluation\n\nIn this notebook, I will evaluate the models on data that has never been seen before and comes from a different distribution than the training, validation, and testing data. This will give me insight on the top model performances.\n\nIn the MVP, I utilized a basic custom Transformer Model. In the newest version, I utilized a DistilBERT model. After sending both versions to people, one piece of feedback I got was that the Transformer Model was better. This is definitely a surprising finding since the DistilBERT model performed better on all my metrics in comparison to the Transformer. Thus, I am going to take this completely different dataset and see how both models perform.","metadata":{}},{"cell_type":"code","source":"# Getting the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom nltk.stem import SnowballStemmer\nimport re\nimport tensorflow as tf\nimport math\n\ntqdm.pandas()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:22:35.147084Z","iopub.execute_input":"2024-02-05T23:22:35.147463Z","iopub.status.idle":"2024-02-05T23:22:52.608704Z","shell.execute_reply.started":"2024-02-05T23:22:35.147431Z","shell.execute_reply":"2024-02-05T23:22:52.607728Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-05 23:22:43.342668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-05 23:22:43.342785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-05 23:22:43.471076: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Getting the data\ntest_data = pd.read_csv('../input/training-llm-competition/hcV3-imagined-stories-with-generated.csv')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:22:57.165723Z","iopub.execute_input":"2024-02-05T23:22:57.166359Z","iopub.status.idle":"2024-02-05T23:22:57.436731Z","shell.execute_reply.started":"2024-02-05T23:22:57.166329Z","shell.execute_reply":"2024-02-05T23:22:57.435750Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                     AssignmentId  \\\n0  32RIADZISTQWI5XIVG5BN0VMYFRS4U   \n1  3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61   \n2  3MTMREQS4W44RBU8OMP3XSK8NMJAWZ   \n3  36WLNQG780WFTLD990VT6XXEYVQEBZ   \n4  32Z9ZLUT1M6BWPTK368LXKUQWLLOHY   \n\n                                               story  \\\n0  Concerts are my most favorite thing, and my bo...   \n1  It seems just like yesterday but today makes f...   \n2  About a month ago I went to burning man. I was...   \n3  Play stupid games, win stupid prizes road trip...   \n4  I wanted to write about one of the best days i...   \n\n                                             summary  timeSinceEvent  \\\n0  My boyfriend and I went to a concert together ...              90   \n1  My sister gave birth to my twin niece and neph...             150   \n2  It is always a journey for me to go to burning...              30   \n3  What happened is that I was on a trip with my ...              90   \n4  Me and my girlfriend went to the zoo on a hot ...              30   \n\n                                     generated_story  \n0  I had been eagerly anticipating this concert f...  \n1  I can hardly contain my excitement as I make m...  \n2  It’s been a whirlwind since I returned from Bu...  \n3  I can't believe it's been three months since t...  \n4  Today was such a hot day, but that didn't stop...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AssignmentId</th>\n      <th>story</th>\n      <th>summary</th>\n      <th>timeSinceEvent</th>\n      <th>generated_story</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32RIADZISTQWI5XIVG5BN0VMYFRS4U</td>\n      <td>Concerts are my most favorite thing, and my bo...</td>\n      <td>My boyfriend and I went to a concert together ...</td>\n      <td>90</td>\n      <td>I had been eagerly anticipating this concert f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61</td>\n      <td>It seems just like yesterday but today makes f...</td>\n      <td>My sister gave birth to my twin niece and neph...</td>\n      <td>150</td>\n      <td>I can hardly contain my excitement as I make m...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3MTMREQS4W44RBU8OMP3XSK8NMJAWZ</td>\n      <td>About a month ago I went to burning man. I was...</td>\n      <td>It is always a journey for me to go to burning...</td>\n      <td>30</td>\n      <td>It’s been a whirlwind since I returned from Bu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>36WLNQG780WFTLD990VT6XXEYVQEBZ</td>\n      <td>Play stupid games, win stupid prizes road trip...</td>\n      <td>What happened is that I was on a trip with my ...</td>\n      <td>90</td>\n      <td>I can't believe it's been three months since t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32Z9ZLUT1M6BWPTK368LXKUQWLLOHY</td>\n      <td>I wanted to write about one of the best days i...</td>\n      <td>Me and my girlfriend went to the zoo on a hot ...</td>\n      <td>30</td>\n      <td>Today was such a hot day, but that didn't stop...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Getting the generated and non-generated ones\nnon_generated = pd.DataFrame()\nnon_generated['story'] = test_data['story']\nnon_generated['label'] = 0.0\n\ngenerated = pd.DataFrame()\ngenerated['story'] = test_data['generated_story']\ngenerated['label'] = 1.0\n\ntesting_data = pd.concat([non_generated,generated])\ntesting_data","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:22:58.012672Z","iopub.execute_input":"2024-02-05T23:22:58.013100Z","iopub.status.idle":"2024-02-05T23:22:58.043171Z","shell.execute_reply.started":"2024-02-05T23:22:58.013068Z","shell.execute_reply":"2024-02-05T23:22:58.042103Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                  story  label\n0     Concerts are my most favorite thing, and my bo...    0.0\n1     It seems just like yesterday but today makes f...    0.0\n2     About a month ago I went to burning man. I was...    0.0\n3     Play stupid games, win stupid prizes road trip...    0.0\n4     I wanted to write about one of the best days i...    0.0\n...                                                 ...    ...\n2751  I can't believe it. Today was my oldest daught...    1.0\n2752  It's hard to believe that it has been 150 days...    1.0\n2753  It's been five long years since my mother pass...    1.0\n2754  I can hardly believe that four months have alr...    1.0\n2755  It feels like just yesterday, but it's been al...    1.0\n\n[5512 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>story</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Concerts are my most favorite thing, and my bo...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>It seems just like yesterday but today makes f...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>About a month ago I went to burning man. I was...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Play stupid games, win stupid prizes road trip...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I wanted to write about one of the best days i...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2751</th>\n      <td>I can't believe it. Today was my oldest daught...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2752</th>\n      <td>It's hard to believe that it has been 150 days...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2753</th>\n      <td>It's been five long years since my mother pass...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2754</th>\n      <td>I can hardly believe that four months have alr...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2755</th>\n      <td>It feels like just yesterday, but it's been al...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5512 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Getting the training and validation datasets\ntrain_data = pd.read_csv('../input/training-llm-competition/train.csv')\nvalid_data = pd.read_csv('../input/training-llm-competition/validation.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:22:58.798413Z","iopub.execute_input":"2024-02-05T23:22:58.798855Z","iopub.status.idle":"2024-02-05T23:23:06.430781Z","shell.execute_reply.started":"2024-02-05T23:22:58.798809Z","shell.execute_reply":"2024-02-05T23:23:06.429745Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## DistilBERT","metadata":{}},{"cell_type":"code","source":"# Getting the model\ntokenizer = AutoTokenizer.from_pretrained(\"Skittles2821/distilbert-detector\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Skittles2821/distilbert-detector\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:23:09.369860Z","iopub.execute_input":"2024-02-05T23:23:09.370771Z","iopub.status.idle":"2024-02-05T23:23:13.060814Z","shell.execute_reply.started":"2024-02-05T23:23:09.370726Z","shell.execute_reply":"2024-02-05T23:23:13.059738Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34289197eb8b43a2a270d0b960dfec76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f8f3e44e9e41e6b9a6d6fcce51f427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57e6fe205df402cbe8093db4d7220d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1ee4e081734d0f81a70088e7f4095f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30a50c3199a745cf95a7501bf3b940da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0ee3f9e2ff740f5bc3c44cb3904bf6e"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Preprocessing\ndef preprocess(essay:str):\n    preprocessed_essay = essay.lower()\n    \n    # Subbing out \\n and \\t\n    preprocessed_essay = re.sub(\"\\n\",\"\",preprocessed_essay)\n    preprocessed_essay = re.sub(\"\\t\",\"\",preprocessed_essay)\n\n    # Replacing /xa0 = non-breaking space in Latin1\n    preprocessed_essay = preprocessed_essay.replace(u'\\xa0', u' ')\n    \n    return preprocessed_essay","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:23:14.061779Z","iopub.execute_input":"2024-02-05T23:23:14.062766Z","iopub.status.idle":"2024-02-05T23:23:14.068450Z","shell.execute_reply.started":"2024-02-05T23:23:14.062730Z","shell.execute_reply":"2024-02-05T23:23:14.067445Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"processed_essays_train = train_data['essay'].copy().progress_apply(preprocess)\nprocessed_essays_valid = valid_data['essay'].copy().progress_apply(preprocess)\nprocessed_essays_test = testing_data['story'].copy().progress_apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:23:15.167873Z","iopub.execute_input":"2024-02-05T23:23:15.168794Z","iopub.status.idle":"2024-02-05T23:23:16.374611Z","shell.execute_reply.started":"2024-02-05T23:23:15.168751Z","shell.execute_reply":"2024-02-05T23:23:16.373654Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 44733/44733 [00:00<00:00, 58337.45it/s]\n100%|██████████| 5195/5195 [00:00<00:00, 14464.72it/s]\n100%|██████████| 5512/5512 [00:00<00:00, 91023.64it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Defining a function for inference\ndef inference(essay:str) -> float:\n    # Tokenizing the input essay\n    inputs = tokenizer(essay,padding='max_length',truncation=True,max_length=512,return_tensors='pt').to(device)\n    \n    # Getting the logits\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        probability = nn.functional.sigmoid(logits)\n    return probability.item()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:23:17.224889Z","iopub.execute_input":"2024-02-05T23:23:17.225290Z","iopub.status.idle":"2024-02-05T23:23:17.232006Z","shell.execute_reply.started":"2024-02-05T23:23:17.225261Z","shell.execute_reply":"2024-02-05T23:23:17.230526Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Getting the predictions\npredictions_train = processed_essays_train.progress_apply(inference)\npredictions_valid = processed_essays_valid.progress_apply(inference)\npredictions_test = processed_essays_test.progress_apply(inference)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:23:18.489842Z","iopub.execute_input":"2024-02-05T23:23:18.491173Z","iopub.status.idle":"2024-02-05T23:40:29.197283Z","shell.execute_reply.started":"2024-02-05T23:23:18.491129Z","shell.execute_reply":"2024-02-05T23:40:29.196346Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 44733/44733 [13:42<00:00, 54.36it/s]\n100%|██████████| 5195/5195 [01:45<00:00, 49.33it/s]\n100%|██████████| 5512/5512 [01:42<00:00, 53.77it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Making predictions\nprint('DistilBERT Score')\nprint(f'Training ROC AUC: {roc_auc_score(train_data[\"LLM_written\"],predictions_train)}')\nprint(f'Validation ROC AUC: {roc_auc_score(valid_data[\"LLM_written\"],predictions_valid)}')\nprint(f'Testing ROC AUC: {roc_auc_score(testing_data[\"label\"],predictions_test)}')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:41:41.334805Z","iopub.execute_input":"2024-02-05T23:41:41.335252Z","iopub.status.idle":"2024-02-05T23:41:41.371197Z","shell.execute_reply.started":"2024-02-05T23:41:41.335213Z","shell.execute_reply":"2024-02-05T23:41:41.370201Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"DistilBERT Score\nTraining ROC AUC: 0.9999537707382391\nValidation ROC AUC: 0.9710747774141688\nTesting ROC AUC: 0.9778706861503915\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Custom Transformer","metadata":{}},{"cell_type":"code","source":"contractions = {\n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:41:53.092152Z","iopub.execute_input":"2024-02-05T23:41:53.092579Z","iopub.status.idle":"2024-02-05T23:41:53.108883Z","shell.execute_reply.started":"2024-02-05T23:41:53.092546Z","shell.execute_reply":"2024-02-05T23:41:53.107953Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Getting the tokenizer\ntokenizer = get_tokenizer('spacy',language='en_core_web_sm')\n\n# Getting the stemmer\nstemmer = SnowballStemmer(language='english')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:41:53.894280Z","iopub.execute_input":"2024-02-05T23:41:53.894635Z","iopub.status.idle":"2024-02-05T23:41:59.132260Z","shell.execute_reply.started":"2024-02-05T23:41:53.894607Z","shell.execute_reply":"2024-02-05T23:41:59.131333Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# A function for preprocessing\ndef preprocess(essay:str):\n    preprocessed_essay = essay.lower()\n    \n    \n    # Iterating through the contractions and replacing the \n    for contraction in contractions.keys():\n        preprocessed_essay = re.sub(contraction.lower(),contractions[contraction].lower(),preprocessed_essay)\n    \n    # Subbing out \\n and \\t\n    preprocessed_essay = re.sub(\"\\n\",\"\",preprocessed_essay)\n    preprocessed_essay = re.sub(\"\\t\",\"\",preprocessed_essay)\n\n    # Replacing /xa0 = non-breaking space in Latin1\n    preprocessed_essay = preprocessed_essay.replace(u'\\xa0', u' ')\n    \n    final_preprocessed_essay = []\n    \n    # Running through tokenizer and returning the non-whitespace tokens\n    for token in tokenizer(preprocessed_essay):\n        temp_token = token.strip(\" \")\n        \n        if temp_token != \"\":\n            final_preprocessed_essay.append(stemmer.stem(token))\n    \n    return final_preprocessed_essay","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:42:01.398195Z","iopub.execute_input":"2024-02-05T23:42:01.399711Z","iopub.status.idle":"2024-02-05T23:42:01.409588Z","shell.execute_reply.started":"2024-02-05T23:42:01.399648Z","shell.execute_reply":"2024-02-05T23:42:01.408429Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Running the training essays and validation essays through preprocessing\ntokenized_essays_train = train_data['essay'].copy().progress_apply(preprocess)\ntokenized_essays_valid = valid_data['essay'].copy().progress_apply(preprocess)\ntokenized_essays_test = testing_data['story'].copy().progress_apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:42:02.781425Z","iopub.execute_input":"2024-02-05T23:42:02.781928Z","iopub.status.idle":"2024-02-05T23:51:14.762358Z","shell.execute_reply.started":"2024-02-05T23:42:02.781894Z","shell.execute_reply":"2024-02-05T23:51:14.761385Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 44733/44733 [06:41<00:00, 111.52it/s]\n100%|██████████| 5195/5195 [01:49<00:00, 47.64it/s] \n100%|██████████| 5512/5512 [00:41<00:00, 131.89it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the vocab\nvocabulary = torch.load('../input/llm-competition-models/vocab.pt')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:32.528200Z","iopub.execute_input":"2024-02-05T23:51:32.528838Z","iopub.status.idle":"2024-02-05T23:51:32.665859Z","shell.execute_reply.started":"2024-02-05T23:51:32.528805Z","shell.execute_reply":"2024-02-05T23:51:32.664999Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Function to put each essay through the vocabulary\ndef put_through_vocab(essay:str) -> list:\n    return vocabulary(essay)\n\n# Indexed\nindexed_essays_train = [put_through_vocab(essay) for essay in tokenized_essays_train]\nindexed_essays_valid = [put_through_vocab(essay) for essay in tokenized_essays_valid]\nindexed_essays_test = [put_through_vocab(essay) for essay in tokenized_essays_test]","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:34.499582Z","iopub.execute_input":"2024-02-05T23:51:34.499992Z","iopub.status.idle":"2024-02-05T23:51:38.656046Z","shell.execute_reply.started":"2024-02-05T23:51:34.499959Z","shell.execute_reply":"2024-02-05T23:51:38.654954Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_padded = tf.keras.utils.pad_sequences(indexed_essays_train,maxlen=512,padding='post',truncating='post',value=vocabulary['<pad>'])\nvalid_padded = tf.keras.utils.pad_sequences(indexed_essays_valid,maxlen=512,padding='post',truncating='post',value=vocabulary['<pad>'])\ntest_padded = tf.keras.utils.pad_sequences(indexed_essays_test,maxlen=512,padding='post',truncating='post',value=vocabulary['<pad>'])","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:40.772724Z","iopub.execute_input":"2024-02-05T23:51:40.773115Z","iopub.status.idle":"2024-02-05T23:51:42.969128Z","shell.execute_reply.started":"2024-02-05T23:51:40.773085Z","shell.execute_reply":"2024-02-05T23:51:42.968106Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self,emb_size:int, dropout:float, maxlen:int = 500):\n        super(PositionalEncoding,self).__init__()\n        den = torch.exp(-torch.arange(0,emb_size,2)*math.log(10000) / emb_size)\n        pos = torch.arange(0,maxlen).reshape(maxlen,1)\n        pos_embedding = torch.zeros((maxlen,emb_size))\n        pos_embedding[:,0::2] = torch.sin(pos * den)\n        pos_embedding[:,1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(0)\n\n        self.dropout = nn.Dropout(dropout)\n\n        # Saving the positional encoding in the model state dict, but making sure PyTorch doesn't \"train\"\n        # these parameters because they don't need to be trained\n        self.register_buffer('pos_embedding',pos_embedding)\n\n    def forward(self,token_embedding):\n        return self.dropout(token_embedding + self.pos_embedding)\n\n# Transformer Model\nclass Model(nn.Module):\n    def __init__(self,vocab_size: int, emb_size:int,nheads:int,dim_feedforward:int,dropout:float,num_layers:int,max_length:int):\n        super().__init__()\n        self.embed_size = emb_size\n        self.embedding = nn.Embedding(vocab_size,emb_size,padding_idx=vocabulary['<pad>'])\n        self.positional_encoder = PositionalEncoding(emb_size,dropout,max_length)\n        self.encoder_layer = nn.TransformerEncoderLayer(emb_size,nheads,dim_feedforward,dropout,batch_first=True)\n        self.transformer = nn.TransformerEncoder(self.encoder_layer,num_layers)\n        self.fc1 = nn.Linear(emb_size,1)\n    \n    # Forward Function\n    def forward(self,X,src_key_padding_mask):\n        # Putting X through embedding\n        output = self.embedding(X.long()) * math.sqrt(self.embed_size)\n        output = self.positional_encoder(output)\n        \n        # Feeding through transformer encoder\n        output = self.transformer(output,src_key_padding_mask=src_key_padding_mask)\n        output = torch.mean(output,dim=1)\n        return nn.functional.sigmoid(self.fc1(output))","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:45.482148Z","iopub.execute_input":"2024-02-05T23:51:45.482489Z","iopub.status.idle":"2024-02-05T23:51:45.496005Z","shell.execute_reply.started":"2024-02-05T23:51:45.482464Z","shell.execute_reply":"2024-02-05T23:51:45.494993Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Creating a mask to make sure the padding indicies are masked\ndef create_padding_mask(X):\n    return (X == vocabulary['<pad>'])","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:46.514434Z","iopub.execute_input":"2024-02-05T23:51:46.514806Z","iopub.status.idle":"2024-02-05T23:51:46.519602Z","shell.execute_reply.started":"2024-02-05T23:51:46.514777Z","shell.execute_reply":"2024-02-05T23:51:46.518551Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Setting up the model\nmodel = Model(vocab_size=vocabulary.__len__(),emb_size=512,nheads=8,dim_feedforward=2048,dropout=0.2,num_layers=2,max_length=512)\n\nmodel.load_state_dict(torch.load('../input/llm-competition-models/2-layer-transformer-encoder.pt'))\n\n# Putting all on GPU\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:47.216570Z","iopub.execute_input":"2024-02-05T23:51:47.216987Z","iopub.status.idle":"2024-02-05T23:51:48.609578Z","shell.execute_reply.started":"2024-02-05T23:51:47.216955Z","shell.execute_reply":"2024-02-05T23:51:48.607647Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Model(\n  (embedding): Embedding(25002, 512, padding_idx=1)\n  (positional_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n  (encoder_layer): TransformerEncoderLayer(\n    (self_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n    )\n    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (dropout1): Dropout(p=0.2, inplace=False)\n    (dropout2): Dropout(p=0.2, inplace=False)\n  )\n  (transformer): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.2, inplace=False)\n        (dropout2): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (fc1): Linear(in_features=512, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Make a function for inference\ndef inference(X,y):\n    X_torch = torch.from_numpy(X)\n    y_torch = torch.from_numpy(y)\n    dataset = TensorDataset(X_torch,y_torch)\n    dataloader = DataLoader(dataset,batch_size=128,shuffle=True)\n    with torch.no_grad():\n        model.eval()\n        preds = None\n        targets = None\n        for X,y in dataloader:\n            # Making predictions\n            X = X.to(device)\n            pred = model(X,src_key_padding_mask=create_padding_mask(X))\n            if preds is None:\n                preds = pred.cpu().detach().numpy()\n            else:\n                preds = np.append(preds,pred.cpu().detach().numpy(),axis=0)\n\n            # Getting the targets\n            if targets is None:\n                targets = y.cpu().numpy()\n            else:\n                targets = np.append(targets,y.cpu().detach().numpy(),axis=0)\n    \n    return preds, targets","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:49.295340Z","iopub.execute_input":"2024-02-05T23:51:49.295939Z","iopub.status.idle":"2024-02-05T23:51:49.304537Z","shell.execute_reply.started":"2024-02-05T23:51:49.295908Z","shell.execute_reply":"2024-02-05T23:51:49.303467Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Making Predictions\ntrain_preds, train_targets = inference(train_padded,train_data['LLM_written'].values)\nvalid_preds, valid_targets = inference(valid_padded,valid_data['LLM_written'].values)\ntest_preds, test_targets = inference(test_padded,testing_data['label'].values)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:51:49.901696Z","iopub.execute_input":"2024-02-05T23:51:49.902353Z","iopub.status.idle":"2024-02-05T23:53:18.997766Z","shell.execute_reply.started":"2024-02-05T23:51:49.902325Z","shell.execute_reply":"2024-02-05T23:53:18.996959Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Making predictions\nprint('Transformer Score')\nprint(f'Training ROC AUC: {roc_auc_score(train_targets,train_preds)}')\nprint(f'Validation ROC AUC: {roc_auc_score(valid_targets,valid_preds)}')\nprint(f'Testing ROC AUC: {roc_auc_score(test_targets,test_preds)}')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:53:57.984231Z","iopub.execute_input":"2024-02-05T23:53:57.984625Z","iopub.status.idle":"2024-02-05T23:53:58.026652Z","shell.execute_reply.started":"2024-02-05T23:53:57.984592Z","shell.execute_reply":"2024-02-05T23:53:58.025381Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Transformer Score\nTraining ROC AUC: 0.9983653983570595\nValidation ROC AUC: 0.8873101185764299\nTesting ROC AUC: 0.5977724284369135\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Summary: This experiment proves that DistilBERT generalizes a lot better than the Transformer. With this in mind, I should produce a prediction instead of a probability. I need to find the decision threshold. From my analysis, 0.5 isn't a good one. ","metadata":{}},{"cell_type":"code","source":"# Getting the TPR,FPR, and Thresholds for the DistilBERT model\ntrain_fpr,train_tpr,train_thresholds = roc_curve(train_data[\"LLM_written\"],predictions_train)\nvalid_fpr,valid_tpr,valid_thresholds = roc_curve(valid_data[\"LLM_written\"],predictions_valid)\ntest_fpr,test_tpr,test_thresholds = roc_curve(testing_data[\"label\"],predictions_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:54:03.343367Z","iopub.execute_input":"2024-02-05T23:54:03.344005Z","iopub.status.idle":"2024-02-05T23:54:03.365130Z","shell.execute_reply.started":"2024-02-05T23:54:03.343974Z","shell.execute_reply":"2024-02-05T23:54:03.364247Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"training_metrics = []\nvalid_metrics = []\ntest_metrics = []\n\nfor metric in zip(train_fpr,train_tpr,train_thresholds,train_tpr-train_fpr):\n    training_metrics.append(metric)\n\nfor metric in zip(valid_fpr,valid_tpr,valid_thresholds,valid_tpr-valid_fpr):\n    valid_metrics.append(metric)\n\nfor metric in zip(test_fpr,test_tpr,test_thresholds,test_tpr-test_fpr):\n    test_metrics.append(metric)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:54:05.036870Z","iopub.execute_input":"2024-02-05T23:54:05.037262Z","iopub.status.idle":"2024-02-05T23:54:05.052890Z","shell.execute_reply.started":"2024-02-05T23:54:05.037231Z","shell.execute_reply":"2024-02-05T23:54:05.051850Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Getting the max difference for each\nsorted(training_metrics,key=lambda x: x[3],reverse=True)[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:54:43.966077Z","iopub.execute_input":"2024-02-05T23:54:43.966493Z","iopub.status.idle":"2024-02-05T23:54:43.976786Z","shell.execute_reply.started":"2024-02-05T23:54:43.966460Z","shell.execute_reply":"2024-02-05T23:54:43.975411Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(0.0008781173164734809,\n 0.9983397897066962,\n 0.5422803163528442,\n 0.9974616723902228)"},"metadata":{}}]},{"cell_type":"code","source":"(sorted(valid_metrics,key=lambda x: x[3],reverse=True)[0][2] + sorted(test_metrics,key=lambda x: x[3],reverse=True)[0][2])/2","metadata":{"execution":{"iopub.status.busy":"2024-02-05T23:56:15.126038Z","iopub.execute_input":"2024-02-05T23:56:15.126818Z","iopub.status.idle":"2024-02-05T23:56:15.134734Z","shell.execute_reply.started":"2024-02-05T23:56:15.126785Z","shell.execute_reply":"2024-02-05T23:56:15.133721Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.7122994661331177"},"metadata":{}}]},{"cell_type":"markdown","source":"Looks like the threshold should be 0.71. Anything above 0.71 should be classified as LLM generated and anything below it should be classified as student written.","metadata":{}}]}